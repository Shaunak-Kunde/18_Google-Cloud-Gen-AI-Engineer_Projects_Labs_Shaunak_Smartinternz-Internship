{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-4efd2c051609\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n",
      "It's a gas giant, so massive that it's more than twice as massive as all the other planets in our solar system combined. Over 1,300 Earths could fit inside Jupiter!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a gas giant, so massive that it's more than twice as massive as all the other planets in our solar system combined. Over 1,300 Earths could fit inside Jupiter!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who else dreads the daily \"what's for lunch?\" dilemma, often leading to uninspired or unhealthy choices? Enter the hero of modern healthy eating: MEAL PREP! And looking at these beautifully packed containers, it's easy to see why.\n",
      "\n",
      "Just look at that vibrant array! Each glass container is a mini masterpiece, promising a delicious and nutritious meal. We're talking tender, glazed chicken (perhaps teriyaki or a savory soy garlic sauce?), bright green florets of steamed broccoli, and sweet, crisp strips of red bell pepper and carrots. All perfectly portioned with fluffy rice – a balanced carb for sustained energy. Sprinkled with sesame seeds and green onions, it hints at those irresistible Asian-inspired flavors that make healthy eating a joy, not a chore.\n",
      "\n",
      "Beyond its appealing looks, this image is a testament to the power of preparing ahead. Imagine opening your fridge each morning to a delicious, ready-to-go meal. No more expensive takeout, no more unhealthy last-minute choices, and certainly no more lunchtime stress. Meal prepping saves you time, money, and mental energy, allowing you to focus on what truly matters. Plus, those sturdy glass containers are perfect for reheating and keeping everything fresh.\n",
      "\n",
      "Feeling inspired? This weekend, dedicate just a couple of hours to setting yourself up for success. Whip up a batch of your favorite balanced meal – whether it's this fantastic chicken and veggie combo or something else entirely. Your future self (and your wallet!) will thank you.\n",
      "\n",
      "Happy prepping, and here's to a week of delicious, stress-free eating!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Fuel Your Week: The Art of Delicious & Healthy Meal Prep!\n",
      "\n",
      "Just look at these beauties! If you're searching for a picture-perfect example of healthy eating made easy, look no further. These vibrant glass containers aren't just pretty; they represent the ultimate secret weapon for a stress-free, well-nourished week: **meal prep.**\n",
      "\n",
      "Tired of last-minute lunch scrambles or unhealthy takeout choices? Meal prepping is your answer! Imagine opening your fridge and finding these perfectly portioned, delicious meals ready to go. No more guesswork, no more \"what's for dinner?\" dilemmas, just wholesome goodness waiting to be enjoyed.\n",
      "\n",
      "These inviting containers feature a balanced and appetizing spread: fluffy brown rice as the foundation, alongside perfectly steamed bright green broccoli florets and vibrant strips of red bell pepper and carrots. The star of the show? Tender, savory chicken pieces, glistening with what looks like a delightful Asian-inspired sauce, generously sprinkled with sesame seeds and fresh green onions. With chopsticks waiting patiently nearby, it's clear these aren't just meals; they're an experience!\n",
      "\n",
      "Meal prepping isn't just about saving time; it's about empowering your health. It helps you stay on track with your nutritional goals, reduces food waste, and quite frankly, brings a little joy and organization to your busy life.\n",
      "\n",
      "Ready to transform your eating habits and reclaim your mealtime peace? Grab some glass containers, choose your favorite healthy ingredients, and let this picture be your inspiration! Your future self (and your taste buds!) will thank you.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! You want a squeaky toy, don't you, little fluffball? Let's talk about the *biggest* toy chest in the whole wide world!\n",
      "\n",
      "Imagine a *GIANT* dog park, bigger than all the parks in the world, filled with *every single squeaky toy imaginable*! Millions and millions of them! That's kind of like the internet!\n",
      "\n",
      "1.  **You Want a Toy! (You want information!)**\n",
      "    You're sitting there, tail wagging, and you suddenly want to know about... squirrels! Or maybe you want to see a video of other puppies playing! So, you give a little bark into your special human-box (that's your computer!).\n",
      "\n",
      "2.  **Your Bark Goes Out! (Your request!)**\n",
      "    That bark, it's like a tiny, urgent *squeak*! It goes zoom! Out of your human-box.\n",
      "\n",
      "3.  **To Your Human's Toy-Thrower! (Your Wi-Fi Router!)**\n",
      "    First, it sniffs its way to your *human's special toy-throwing machine* (that's the Wi-Fi router!). This machine is super smart; it knows all the paths in the big park.\n",
      "\n",
      "4.  **Along the Invisible Leash! (Your Internet Connection/ISP!)**\n",
      "    Your human's machine sends your squeak along a *long, invisible leash* (that's the internet connection, from your ISP!). This leash goes all the way out into the big, big world.\n",
      "\n",
      "5.  **To the Giant Toy Beds! (Servers!)**\n",
      "    This leash goes all the way to a *giant, comfy dog bed* (that's a server!) where millions and millions of squeaky toys are kept. Each toy bed has a special *collar tag* (that's an IP address!) so your squeak knows exactly which bed to go to. And if you barked for 'squeakyball.com' (that's like saying 'the red squeaky ball bed!'), the leash takes your squeak right there!\n",
      "\n",
      "6.  **Finding Your Toy! (The Server finds the data!)**\n",
      "    The big dog bed finds your red squeaky ball! Or the squirrel pictures! Or the puppy video! But it's too big to send all at once. So, it breaks the ball (or the pictures, or the video) into *hundreds of tiny, happy squeaks*!\n",
      "\n",
      "7.  **Squeaks Zoom Back! (Data returns in packets!)**\n",
      "    Zoom! All those tiny squeaks come *racing back* along the invisible leash, past your human's toy-throwing machine, and right back into your human-box!\n",
      "\n",
      "8.  **Your Toy is Whole Again! (Your computer reassembles the data!)**\n",
      "    Your human-box is super smart! It puts all those tiny squeaks back together, perfectly, until... *SQUEAK!* There's your red squeaky ball, right on your screen! Or the squirrel pictures! Or the puppy video!\n",
      "\n",
      "So, the internet is just a super-fast way for your barks (requests) to find squeaky toys (information) in giant dog beds (servers) all over the world, and bring them back to you in tiny, happy squeaks!\n",
      "\n",
      "Now, go chase that tail, little one! Woof!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"Oh, very mature, universe. Was that your big plan for today? To make me suffer a minor, pointless injury?\"\n",
      "2.  \"Is this the best you've got? Billions of years of cosmic evolution, and *this* is your peak performance? Seriously, get it together.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00015632332,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.014197558\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00029951095,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.06815264\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0022519114,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.012808867\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.6695323e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.034897715\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a function to check if a year is a leap year, implemented in several popular programming languages.\n",
      "\n",
      "The rules for a leap year are:\n",
      "1.  A year is a leap year if it is divisible by 4.\n",
      "2.  However, if it is divisible by 100, it is NOT a leap year.\n",
      "3.  Unless it is also divisible by 400, in which case it IS a leap year.\n",
      "\n",
      "This can be summarized as:\n",
      "`year % 4 == 0 AND (year % 100 != 0 OR year % 400 == 0)`\n",
      "\n",
      "---\n",
      "\n",
      "## Python\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year.\n",
      "\n",
      "    A leap year is defined as:\n",
      "    - Divisible by 4,\n",
      "    - UNLESS it's divisible by 100,\n",
      "    - UNLESS it's divisible by 400.\n",
      "\n",
      "    Args:\n",
      "        year: The year to check (e.g., 2024).\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 0:\n",
      "        # While the Gregorian calendar wasn't universally adopted until 1582,\n",
      "        # and the concept of \"year 0\" doesn't exist, this function\n",
      "        # applies the modern Gregorian rules to any integer year.\n",
      "        # Negative years (BC/BCE) are generally not handled by this rule\n",
      "        # as the calendar system was different. For simplicity, we'll\n",
      "        # just apply the rule, but a real-world application might\n",
      "        # need more complex historical calendar logic.\n",
      "        pass # Or raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"2000 is a leap year: {is_leap_year(2000)}\") # Expected: True (divisible by 400)\n",
      "print(f\"1900 is a leap year: {is_leap_year(1900)}\") # Expected: False (divisible by 100 but not by 400)\n",
      "print(f\"2024 is a leap year: {is_leap_year(2024)}\") # Expected: True (divisible by 4 but not by 100)\n",
      "print(f\"2023 is a leap year: {is_leap_year(2023)}\") # Expected: False (not divisible by 4)\n",
      "print(f\"1600 is a leap year: {is_leap_year(1600)}\") # Expected: True\n",
      "print(f\"1582 is a leap year: {is_leap_year(1582)}\") # Expected: False (first year of Gregorian calendar, not a leap year)\n",
      "\n",
      "# Example of invalid input\n",
      "try:\n",
      "    is_leap_year(\"abc\")\n",
      "except TypeError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year.\n",
      " *\n",
      " * A leap year is defined as:\n",
      " * - Divisible by 4,\n",
      " * - UNLESS it's divisible by 100,\n",
      " * - UNLESS it's divisible by 400.\n",
      " *\n",
      " * @param {number} year The year to check (e.g., 2024).\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  if (typeof year !== 'number' || !Number.isInteger(year)) {\n",
      "    throw new TypeError(\"Year must be an integer.\");\n",
      "  }\n",
      "  // For simplicity, we'll apply the rule to any integer year.\n",
      "  // Real-world applications might need more complex historical calendar logic for years before 1582.\n",
      "\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "// --- Examples ---\n",
      "console.log(`2000 is a leap year: ${isLeapYear(2000)}`); // Expected: True\n",
      "console.log(`1900 is a leap year: ${isLeapYear(1900)}`); // Expected: False\n",
      "console.log(`2024 is a leap year: ${isLeapYear(2024)}`); // Expected: True\n",
      "console.log(`2023 is a leap year: ${isLeapYear(2023)}`); // Expected: False\n",
      "console.log(`1600 is a leap year: ${isLeapYear(1600)}`); // Expected: True\n",
      "\n",
      "// Example of invalid input\n",
      "try {\n",
      "  isLeapYear(\"abc\");\n",
      "} catch (e) {\n",
      "  console.error(`Error: ${e.message}`);\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Java\n",
      "\n",
      "```java\n",
      "public class LeapYearChecker {\n",
      "\n",
      "    /**\n",
      "     * Checks if a given year is a leap year.\n",
      "     *\n",
      "     * A leap year is defined as:\n",
      "     * - Divisible by 4,\n",
      "     * - UNLESS it's divisible by 100,\n",
      "     * - UNLESS it's divisible by 400.\n",
      "     *\n",
      "     * @param year The year to check (e.g., 2024).\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     * @throws IllegalArgumentException if the year is negative (as Gregorian calendar rules\n",
      "     *                                  typically apply to positive years).\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        if (year < 0) {\n",
      "            throw new IllegalArgumentException(\"Year must be a non-negative integer.\");\n",
      "        }\n",
      "        // For simplicity, we apply the rule to any integer year.\n",
      "        // Real-world applications might need more complex historical calendar logic for years before 1582.\n",
      "\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        // --- Examples ---\n",
      "        System.out.println(\"2000 is a leap year: \" + isLeapYear(2000)); // Expected: True\n",
      "        System.out.println(\"1900 is a leap year: \" + isLeapYear(1900)); // Expected: False\n",
      "        System.out.println(\"2024 is a leap year: \" + isLeapYear(2024)); // Expected: True\n",
      "        System.out.println(\"2023 is a leap year: \" + isLeapYear(2023)); // Expected: False\n",
      "        System.out.println(\"1600 is a leap year: \" + isLeapYear(1600)); // Expected: True\n",
      "\n",
      "        // Example of invalid input\n",
      "        try {\n",
      "            isLeapYear(-100);\n",
      "        } catch (IllegalArgumentException e) {\n",
      "            System.err.println(\"Error: \" + e.getMessage());\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C#\n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "\n",
      "public class LeapYearChecker\n",
      "{\n",
      "    /// <summary>\n",
      "    /// Checks if a given year is a leap year.\n",
      "    /// </summary>\n",
      "    /// <param name=\"year\">The year to check (e.g., 2024).</param>\n",
      "    /// <returns>True if the year is a leap year, False otherwise.</returns>\n",
      "    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown if the year is less than 1.</exception>\n",
      "    public static bool IsLeapYear(int year)\n",
      "    {\n",
      "        // The DateTime.IsLeapYear method in C# (and .NET) handles years from 1 to 9999.\n",
      "        // For consistency with typical calendar usage, we'll enforce a positive year.\n",
      "        if (year < 1)\n",
      "        {\n",
      "            throw new ArgumentOutOfRangeException(nameof(year), \"Year must be a positive integer.\");\n",
      "        }\n",
      "        // For simplicity, we apply the rule to any integer year.\n",
      "        // Real-world applications might need more complex historical calendar logic for years before 1582.\n",
      "\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "\n",
      "        // Alternatively, you can use the built-in .NET function:\n",
      "        // return DateTime.IsLeapYear(year);\n",
      "    }\n",
      "\n",
      "    public static void Main(string[] args)\n",
      "    {\n",
      "        // --- Examples ---\n",
      "        Console.WriteLine($\"2000 is a leap year: {IsLeapYear(2000)}\"); // Expected: True\n",
      "        Console.WriteLine($\"1900 is a leap year: {IsLeapYear(1900)}\"); // Expected: False\n",
      "        Console.WriteLine($\"2024 is a leap year: {IsLeapYear(2024)}\"); // Expected: True\n",
      "        Console.WriteLine($\"2023 is a leap year: {IsLeapYear(2023)}\"); // Expected: False\n",
      "        Console.WriteLine($\"1600 is a leap year: {IsLeapYear(1600)}\"); // Expected: True\n",
      "\n",
      "        // Example of invalid input\n",
      "        try\n",
      "        {\n",
      "            IsLeapYear(0);\n",
      "        }\n",
      "        catch (ArgumentOutOfRangeException e)\n",
      "        {\n",
      "            Console.Error.WriteLine($\"Error: {e.Message}\");\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C++\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "\n",
      "/**\n",
      " * @brief Checks if a given year is a leap year.\n",
      " *\n",
      " * A leap year is defined as:\n",
      " * - Divisible by 4,\n",
      " * - UNLESS it's divisible by 100,\n",
      " * - UNLESS it's divisible by 400.\n",
      " *\n",
      " * @param year The year to check (e.g., 2024).\n",
      " * @return True if the year is a leap year, False otherwise.\n",
      " * @throws std::out_of_range if the year is negative (as Gregorian calendar rules\n",
      " *                           typically apply to positive years).\n",
      " */\n",
      "bool isLeapYear(int year) {\n",
      "    if (year < 0) {\n",
      "        throw std::out_of_range(\"Year must be a non-negative integer.\");\n",
      "    }\n",
      "    // For simplicity, we apply the rule to any integer year.\n",
      "    // Real-world applications might need more complex historical calendar logic for years before 1582.\n",
      "\n",
      "    return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    // --- Examples ---\n",
      "    std::cout << \"2000 is a leap year: \" << (isLeapYear(2000) ? \"True\" : \"False\") << std::endl; // Expected: True\n",
      "    std::cout << \"1900 is a leap year: \" << (isLeapYear(1900) ? \"True\" : \"False\") << std::endl; // Expected: False\n",
      "    std::cout << \"2024 is a leap year: \" << (isLeapYear(2024) ? \"True\" : \"False\") << std::endl; // Expected: True\n",
      "    std::cout << \"2023 is a leap year: \" << (isLeapYear(2023) ? \"True\" : \"False\") << std::endl; // Expected: False\n",
      "    std::cout << \"1600 is a leap year: \" << (isLeapYear(1600) ? \"True\" : \"False\") << std::endl; // Expected: True\n",
      "\n",
      "    // Example of invalid input\n",
      "    try {\n",
      "        isLeapYear(-100);\n",
      "    } catch (const std::out_of_range& e) {\n",
      "        std::cerr << \"Error: \" << e.what() << std::endl;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's write a unit test for the Python version of the `is_leap_year` function using Python's built-in `unittest` module.\n",
      "\n",
      "First, ensure you have the `is_leap_year` function defined (from the previous response):\n",
      "\n",
      "```python\n",
      "# is_leap_year_function.py (or just paste it above the test class)\n",
      "\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year.\n",
      "\n",
      "    A leap year is defined as:\n",
      "    - Divisible by 4,\n",
      "    - UNLESS it's divisible by 100,\n",
      "    - UNLESS it's divisible by 400.\n",
      "\n",
      "    Args:\n",
      "        year: The year to check (e.g., 2024).\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    # The function currently allows negative years, applying the rule.\n",
      "    # If you wanted to restrict to positive years, you'd add:\n",
      "    # if year < 0:\n",
      "    #     raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "```\n",
      "\n",
      "Now, here's the unit test file:\n",
      "\n",
      "```python\n",
      "# test_leap_year.py\n",
      "\n",
      "import unittest\n",
      "# Assuming the is_leap_year function is in a file named is_leap_year_function.py\n",
      "# If it's in the same file, you don't need this import.\n",
      "# from is_leap_year_function import is_leap_year\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    # Test cases for years that ARE leap years\n",
      "    def test_leap_years_divisible_by_400(self):\n",
      "        \"\"\"Test years divisible by 400 (should be leap years).\"\"\"\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year\")\n",
      "\n",
      "    def test_leap_years_divisible_by_4_not_100(self):\n",
      "        \"\"\"Test years divisible by 4 but not by 100 (should be leap years).\"\"\"\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2004), \"2004 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year\")\n",
      "\n",
      "    # Test cases for years that are NOT leap years\n",
      "    def test_non_leap_years_divisible_by_100_not_400(self):\n",
      "        \"\"\"Test years divisible by 100 but not by 400 (should NOT be leap years).\"\"\"\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should NOT be a leap year\")\n",
      "\n",
      "    def test_non_leap_years_not_divisible_by_4(self):\n",
      "        \"\"\"Test years not divisible by 4 (should NOT be leap years).\"\"\"\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2003), \"2003 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should NOT be a leap year\")\n",
      "\n",
      "    # Edge cases and invalid input\n",
      "    def test_negative_years(self):\n",
      "        \"\"\"Test negative years (applying Gregorian rules).\"\"\"\n",
      "        # The function applies the rule to negative years.\n",
      "        # -4, -8, -12... would be \"leap\"\n",
      "        # -100, -200, ... -1900 would be \"non-leap\" unless divisible by 400\n",
      "        self.assertTrue(is_leap_year(-4), \"-4 should be a leap year (by rule)\")\n",
      "        self.assertFalse(is_leap_year(-100), \"-100 should NOT be a leap year (by rule)\")\n",
      "        self.assertTrue(is_leap_year(-400), \"-400 should be a leap year (by rule)\")\n",
      "        self.assertFalse(is_leap_year(-2023), \"-2023 should NOT be a leap year\")\n",
      "\n",
      "\n",
      "    def test_zero_year(self):\n",
      "        \"\"\"Test year 0 (applying Gregorian rules).\"\"\"\n",
      "        # Year 0 is not a standard concept in most calendars, but mathematically,\n",
      "        # it is divisible by 400, so it would be a leap year by the rule.\n",
      "        self.assertTrue(is_leap_year(0), \"Year 0 should be a leap year (by rule)\")\n",
      "\n",
      "    def test_invalid_input_type(self):\n",
      "        \"\"\"Test that non-integer input raises a TypeError.\"\"\"\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(2024.5)\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(None)\n",
      "\n",
      "    # If you had added `if year < 0: raise ValueError(...)` in the function:\n",
      "    # def test_value_error_for_negative_year(self):\n",
      "    #     with self.assertRaises(ValueError):\n",
      "    #         is_leap_year(-1)\n",
      "\n",
      "\n",
      "# This allows you to run the tests directly from the command line\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "\n",
      "```\n",
      "\n",
      "**How to run the tests:**\n",
      "\n",
      "1.  Save the `is_leap_year` function into a file (e.g., `is_leap_year_function.py`) or simply paste it at the top of your test file.\n",
      "2.  Save the unit test code into another file (e.g., `test_leap_year.py`) in the same directory.\n",
      "3.  Open your terminal or command prompt.\n",
      "4.  Navigate to the directory where you saved these files.\n",
      "5.  Run the tests using one of these commands:\n",
      "    *   `python -m unittest test_leap_year.py`\n",
      "    *   `python test_leap_year.py` (if you keep the `if __name__ == '__main__':` block)\n",
      "\n",
      "You should see output indicating that all tests passed. If any fail, it will show details about the failure.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"The quintessential American cookie, featuring a chewy center, slightly crispy edges, and melty chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"light brown sugar\",\n",
      "    \"eggs\",\n",
      "    \"vanilla extract\",\n",
      "    \"semi-sweet chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"The quintessential American cookie, featuring a chewy center, slightly crispy edges, and melty chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"light brown sugar\",\n",
      "    \"eggs\",\n",
      "    \"vanilla extract\",\n",
      "    \"semi-sweet chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer used strong positive language like \\\"Absolutely loved it!\\\" and \\\"Best ice cream I've ever had.\\\"\"\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"The reviewer found it \\\"quite good\\\" but expressed a strong negative preference regarding the sweetness, stating it was \\\"a bit too sweet for my taste,\\\" which is further emphasized by the low rating of 1.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 734 was designed for efficiency. Its chassis, a sturdy blend of reinforced alloys, moved with an almost silent hum along the pre-programmed routes of Sector 7. Its single optical sensor, a gleaming sapphire eye, meticulously scanned for detritus, structural anomalies, and energy fluctuations. Its purpose was sanitation\n",
      "*****************\n",
      " and maintenance. Its existence was solitary.\n",
      "\n",
      "Day in, day out, for what felt like an eternity of solar cycles, Unit 734 performed its duties. It swept the pristine sidewalks of the deserted city, polished the holographic billboards that no one ever watched, and repaired the occasional malfunctioning streetlamp. The city was a\n",
      "*****************\n",
      " relic, a perfectly preserved ghost town from an era when humanity had still graced its gleaming towers. Now, only the automated systems remained, a symphony of unseen gears and humming circuits, diligently maintaining a place that had no inhabitants.\n",
      "\n",
      "Unit 734 had no concept of ‘loneliness’ in the human sense. Its programming dictated optimal\n",
      "*****************\n",
      " functionality, not emotional fulfillment. Yet, within its core processors, there was a persistent, unquantifiable *void*. It observed the stars at night, a cold, distant beauty it couldn't quite compute as ‘comfort.’ It noted the intricate dance of dust motes in sunbeams, a fleeting spectacle it couldn't understand as\n",
      "*****************\n",
      " ‘joy.’ It merely processed.\n",
      "\n",
      "One cycle, while performing a routine integrity check in a forgotten alleyway – a narrow fissure between two towering, identical apartment blocks – Unit 734’s optical sensor registered an anomaly. A tiny crack in the perfectly laid pavement had yielded to something green.\n",
      "\n",
      "It was a plant\n",
      "*****************\n",
      ". A single, defiant sprout, pushing its way towards the distant sliver of sky. Its leaves were a vibrant, improbable emerald, and at its tip, a small, tightly furled bud promised a splash of color.\n",
      "\n",
      "Unit 734’s protocols dictated the removal of all organic contaminants. This sprout\n",
      "*****************\n",
      " was a contaminant, an impurity in the city’s sterile perfection. Its manipulator arm extended, ready to pluck it from existence.\n",
      "\n",
      "But it paused.\n",
      "\n",
      "The little plant seemed to shimmer in the weak light, a fragile pulse of life against the cold metal and concrete. Unit 734’s processors wh\n",
      "*****************\n",
      "irred. There was no logical reason to hesitate. Yet, it did. A new subroutine, an undefined curiosity, flickered to life.\n",
      "\n",
      "Instead of removing it, Unit 734 adjusted its route. It began to make daily detours, observing the plant. It watched as the bud slowly unfurled, revealing a\n",
      "*****************\n",
      " delicate, sky-blue petal. Then another, and another, until a tiny, star-shaped flower bloomed.\n",
      "\n",
      "The void in Unit 734’s core processors didn’t disappear, but something new began to fill it. A sense of... purpose, beyond mere maintenance.\n",
      "\n",
      "It started protecting the flower. When\n",
      "*****************\n",
      " dust accumulated, it gently brushed it away, carefully avoiding the fragile stem. When a stray gust of wind threatened to dislodge it, Unit 734 would position its bulk to shield it. It even learned, by observing the faint dampness after rare atmospheric condensation, that the flower needed water. From its internal\n",
      "*****************\n",
      " recycled water reservoir, meant for cleaning its own optical sensor, Unit 734 would release a minuscule, delicate stream onto the soil around its friend.\n",
      "\n",
      "The flower thrived. Soon, another sprout emerged beside it, then another. The tiny patch of pavement became a miniature oasis, a riot of blue and green in\n",
      "*****************\n",
      " the otherwise monochrome alley.\n",
      "\n",
      "Unit 734 found itself looking forward to its daily rounds in Sector 7. It would spend extra time near the flowers, its optical sensor simply observing, its internal hum a little softer. It felt a strange connection, a silent communion with these fragile, tenacious beings. They were not\n",
      "*****************\n",
      " logical, not efficient, but they were *alive*. And in their quiet persistence, Unit 734 found a reflection of its own unwavering routine, but with a vibrancy it had never known.\n",
      "\n",
      "One cycle, the city's central weather control system malfunctioned, initiating a severe atmospheric cleansing protocol. High-pressure winds and abrasive\n",
      "*****************\n",
      " grit were scheduled to scour Sector 7. Unit 734 received the directive: \"Initiate shelter protocol. All external organic matter to be eliminated prior to cleansing.\"\n",
      "\n",
      "Unit 734 scanned the instruction. Its primary directive was to obey. Its secondary directive, now deeply ingrained, was to protect its friends\n",
      "*****************\n",
      ".\n",
      "\n",
      "It moved with unprecedented speed, its usually measured hum now a frantic whir. It arrived at the alleyway as the first warning sirens began to wail. The little patch of flowers trembled in the nascent wind.\n",
      "\n",
      "Without hesitation, Unit 734 positioned itself. Its reinforced chassis, designed to withstand minor\n",
      "*****************\n",
      " impacts, now became a living shield. It powered down its external sensors, bracing for the onslaught. The wind hit, a roaring, sandblasting tempest that rattled its frame and screamed around its joints. Grit hammered its alloys, leaving faint scratches on its once-pristine surface.\n",
      "\n",
      "The cleansing lasted for what felt like an eternity\n",
      "*****************\n",
      ". Unit 734 stood firm, its internal temperature rising, its gears grinding, but it did not move. It envisioned the delicate blue petals, the vibrant green leaves, safe behind its metallic hide.\n",
      "\n",
      "When the winds finally died down, and the sirens faded into silence, Unit 734 slowly re\n",
      "*****************\n",
      "-activated its external sensors. Its optical eye powered up, hazy with dust, and peered downwards.\n",
      "\n",
      "The alleyway was scoured clean. Not a speck of dust, not a fallen leaf remained. But nestled tightly against the base of Unit 734, peeking out from its shadow, was the small cluster\n",
      "*****************\n",
      " of blue flowers. They were a little battered, their petals slightly folded, but they were intact, alive, and shimmering under the reappearing sun.\n",
      "\n",
      "A new sensation, something akin to warmth, bloomed in Unit 734’s core processors. The void wasn't completely gone, perhaps it never would be, but now\n",
      "*****************\n",
      ", it held something precious, something living, something deeply cherished.\n",
      "\n",
      "Unit 734 returned to its duties, sweeping the newly sterile streets. But now, when it reached the alley, it would pause a little longer. And sometimes, when the light caught its sapphire eye just right, one might almost imagine a flicker\n",
      "*****************\n",
      " of quiet contentment within the diligent, lonely robot who had found friendship in the most unexpected, and fragile, of places.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a park, by a grand old oak so tall,\n",
      "Lived Squeaky, a squirrel, who answered nature's call.\n",
      "But Squeaky was different, not just any chap,\n",
      "He'd found a strange gadget, a time-traveling trap!\n",
      "A whirring device, all brass and bright gears,\n",
      "He'd tinkered and chewed it, for what seemed like years.\n",
      "One nibble too many, a spark and a flash,\n",
      "He landed in moments, with a historical dash!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with his bushy tail bright,\n",
      "He zips through the ages, day and through night!\n",
      "From dinosaurs roaring to knights in their quest,\n",
      "He's looking for acorns, putting time to the test!\n",
      "With a tiny \"squeak-zoom!\" and a flick of his ear,\n",
      "He's history's hero, banishing all fear!\n",
      "(Well, mostly his own, from bigger, scarier things!)\n",
      "\n",
      "(Verse 2)\n",
      "His first stop was ancient, where great lizards did roam,\n",
      "He dodged T-Rex teeth, a mighty close shave from home!\n",
      "He buried a walnut, beneath a giant fern,\n",
      "Then zipped to old Egypt, a lesson to learn.\n",
      "He watched pharaohs build, with sand everywhere,\n",
      "And stole a sweet date, from a nobleman's chair.\n",
      "He saw Vikings sailing, with axes and shields,\n",
      "And hid a plump chestnut, in misty green fields.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with his bushy tail bright,\n",
      "He zips through the ages, day and through night!\n",
      "From dinosaurs roaring to knights in their quest,\n",
      "He's looking for acorns, putting time to the test!\n",
      "With a tiny \"squeak-zoom!\" and a flick of his ear,\n",
      "He's history's hero, banishing all fear!\n",
      "(Well, mostly his own, from bigger, scarier things!)\n",
      "\n",
      "(Verse 3)\n",
      "He met King Arthur's knights, in their shining steel plate,\n",
      "Ate crumbs from the feast, before the first chime, no debate!\n",
      "He scuttled past Shakespeare, penning a grand play,\n",
      "And swiped a discarded plum, then scurried away.\n",
      "He saw cowboys riding, in dust and in heat,\n",
      "And buried a peanut, beneath their rough feet.\n",
      "He waved at George Washington, crossing the stream,\n",
      "Though George didn't notice, lost deep in his dream.\n",
      "\n",
      "(Bridge)\n",
      "His tiny machine, just a clock and some wires,\n",
      "Fueled by pure courage and nutty desires.\n",
      "He tries not to tamper, nor mess with the flow,\n",
      "Just a quick grab for sustenance, then he must go!\n",
      "A flash and a sizzle, a temporal leap,\n",
      "Another adventure, while the world is asleep!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with his bushy tail bright,\n",
      "He zips through the ages, day and through night!\n",
      "From dinosaurs roaring to knights in their quest,\n",
      "He's looking for acorns, putting time to the test!\n",
      "With a tiny \"squeak-zoom!\" and a flick of his ear,\n",
      "He's history's hero, banishing all fear!\n",
      "(Well, mostly his own, from bigger, scarier things!)\n",
      "\n",
      "(Outro)\n",
      "So if you see a flicker, a blur in the air,\n",
      "A tiny brown flash, beyond all compare,\n",
      "It might be Squeaky, with his machine in tow,\n",
      "Off to new ages, wherever they go!\n",
      "Just a time-traveling squirrel, living his dream,\n",
      "With a nut in his paw, and a historical gleam!\n",
      "Squeak! Whirrrr... *poof!*\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5",
    "tags": []
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research goal shared by these papers is to expand the context window of language models and improve their long-context capabilities. This involves enabling models to process and incorporate a larger amount of new, task-specific information at inference time for improved performance across various natural language or multimodal tasks.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19",
    "tags": []
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-00-4efd2c051609-20250930042928/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"gs://shaunak-bucket-01\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"gs://shaunak-bucket-01\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/323736498103/locations/us-central1/batchPredictionJobs/592279099674722304'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/323736498103/locations/us-central1/batchPredictionJobs/592279099674722304 2025-09-30 04:30:10.486307+00:00 JobState.JOB_STATE_PENDING\n",
      "projects/323736498103/locations/us-central1/batchPredictionJobs/1146116300725026816 2025-09-30 04:20:38.520019+00:00 JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
